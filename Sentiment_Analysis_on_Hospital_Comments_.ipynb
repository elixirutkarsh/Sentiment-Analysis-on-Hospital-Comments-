{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOng+k7ef12V+m0gdB8YEaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elixirutkarsh/Sentiment-Analysis-on-Hospital-Comments-/blob/main/Sentiment_Analysis_on_Hospital_Comments_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGiWvGfqnZ04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet2022')\n",
        "\n",
        "! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error."
      ],
      "metadata": {
        "id": "lymVCS1knkPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/private-hospital-comments/comments1.csv\")"
      ],
      "metadata": {
        "id": "mjywtF4SnoLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Main Topic\"] = df[\"Main Topic\"].fillna(\"No Topic Given\")\n",
        "df = df.dropna(subset=[\"Content\"])\n",
        "nan_counts = df.isna().sum()\n",
        "\n",
        "print(nan_counts)"
      ],
      "metadata": {
        "id": "xU7wby7EnorT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining regex patterns.\n",
        "linebreaks        = \"<br /><br />\"\n",
        "alphaPattern      = \"[^a-z0-9<>]\"\n",
        "sequencePattern   = r\"(.)\\1\\1+\"\n",
        "seqReplacePattern = r\"\\1\\1\"\n",
        "\n",
        "# Defining regex for emojis\n",
        "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
        "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
        "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
        "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_reviews(review):\n",
        "\n",
        "    review = review.lower()\n",
        "\n",
        "    review = re.sub(linebreaks,\" \",review)\n",
        "    # Replace 3 or more consecutive letters by 2 letter.\n",
        "    review = re.sub(sequencePattern, seqReplacePattern, review)\n",
        "\n",
        "    # Replace all emojis.\n",
        "    review = re.sub(r'<3', '<heart>', review)\n",
        "    review = re.sub(smileemoji, '<smile>', review)\n",
        "    review = re.sub(sademoji, '<sadface>', review)\n",
        "    review = re.sub(neutralemoji, '<neutralface>', review)\n",
        "    review = re.sub(lolemoji, '<lolface>', review)\n",
        "\n",
        "    # Remove non-alphanumeric and symbols\n",
        "    review = re.sub(alphaPattern, ' ', review)\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokens = word_tokenize(review)\n",
        "\n",
        "    # Remove stop words from the token sequence\n",
        "\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize the remaining tokens\n",
        "    tokens = [Lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join the cleaned tokens into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# combine \"Main Topic\", \"Subtopic\", and \"Content\" columns into a single column called \"Text\"\n",
        "df[\"Text\"] = df[\"Main Topic\"] + \" \" + df[\"Subtopic\"] + \" \" + df[\"Content\"]\n",
        "df[\"Text\"] = df[\"Text\"].apply(preprocess_reviews)"
      ],
      "metadata": {
        "id": "gGXPfRG-nsWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/towardsNLP/IMDB-Semantic-Sentiment-Analysis/main/Word2Vec/src/w2v_utils.py -o w2v_utils.py"
      ],
      "metadata": {
        "id": "S6bVG6pFnye2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from w2v_utils import (Tokenizer,\n",
        "                       evaluate_model,\n",
        "                       bow_vectorizer,\n",
        "                       train_logistic_regressor,\n",
        "                       w2v_trainer,\n",
        "                       calculate_overall_similarity_score,\n",
        "                       overall_semantic_sentiment_analysis,\n",
        "                       list_similarity,\n",
        "                       calculate_topn_similarity_score,\n",
        "                       topn_semantic_sentiment_analysis,\n",
        "                       define_complexity_subjectivity_reviews,\n",
        "                       explore_high_complexity_reviews,\n",
        "                       explore_low_subjectivity_reviews,\n",
        "                       text_SSA)\n",
        "# Instancing the Tokenizer class\n",
        "tokenizer = Tokenizer(clean= True,\n",
        "                      lower= True,\n",
        "                      de_noise= True,\n",
        "                      remove_stop_words= True,\n",
        "                      keep_negation=True)\n",
        "df['tokenized_text'] = df['Text'].apply(tokenizer.tokenize)\n",
        "\n",
        "df['tokenized_text_len'] = df['tokenized_text'].apply(len)\n",
        "df['tokenized_text_len'].apply(np.log).describe()"
      ],
      "metadata": {
        "id": "YLjzGNwonzJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyed_vectors , keyed_vocab = w2v_trainer(df[\"tokenized_text\"])\n",
        "print(type(keyed_vectors))\n",
        "print(type(keyed_vocab))"
      ],
      "metadata": {
        "id": "OMwFzTeqn3eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyed_vectors.most_similar(\"research\",topn=15)"
      ],
      "metadata": {
        "id": "F4mZplLSn5ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyed_vectors.most_similar(\"hospital\",topn=15)"
      ],
      "metadata": {
        "id": "hWAlVXc8n7Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyed_vectors.most_similar(\"funded\",topn=15)"
      ],
      "metadata": {
        "id": "0WE4pOjSn9ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To make sure that all `positive_concepts` are in the keyed word2vec vocabulary\n",
        "positive_concepts = ['excellent', 'awesome', 'cool','decent','amazing', 'strong', 'good', 'great', 'funny', 'entertaining']\n",
        "pos_concepts = [concept for concept in positive_concepts if concept in keyed_vocab]"
      ],
      "metadata": {
        "id": "vaYmAcGon_p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To make sure that all `negative_concepts` are in the keyed word2vec vocabulary\n",
        "negative_concepts = ['terrible','awful','horrible','boring','bad', 'disappointing', 'weak', 'poor',  'senseless','confusing']\n",
        "neg_concepts = [concept for concept in negative_concepts if concept in keyed_vocab]\n",
        "# Calculating Semantic Sentiment Scores by OSSA model\n",
        "overall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n",
        "                                                   positive_target_tokens = pos_concepts,\n",
        "                                                   negative_target_tokens = neg_concepts,\n",
        "                                                   doc_tokens = df['tokenized_text'])\n",
        "\n",
        "# Calculating Semantic Sentiment Scores by TopSSA model\n",
        "topn_df_scores = topn_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n",
        "                                                   positive_target_tokens = pos_concepts,\n",
        "                                                   negative_target_tokens = neg_concepts,\n",
        "                                                   doc_tokens = df['tokenized_text'],\n",
        "                                                     topn=30)\n",
        "\n",
        "\n",
        "# To store semantic sentiment store computed by OSSA model in df\n",
        "df['overall_PSS'] = overall_df_scores[0]\n",
        "df['overall_NSS'] = overall_df_scores[1]\n",
        "df['overall_semantic_sentiment_score'] = overall_df_scores[2]\n",
        "df['overall_semantic_sentiment_polarity'] = overall_df_scores[3]\n",
        "\n",
        "\n",
        "\n",
        "# To store semantic sentiment store computed by TopSSA model in df\n",
        "df['topn_PSS'] = topn_df_scores[0]\n",
        "df['topn_NSS'] = topn_df_scores[1]\n",
        "df['topn_semantic_sentiment_score'] = topn_df_scores[2]\n",
        "df['topn_semantic_sentiment_polarity'] = topn_df_scores[3]\n",
        "words = keyed_vectors.index_to_key\n",
        "vectors = [keyed_vectors[word] for word in words]\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(vectors)\n",
        "\n",
        "# Create a DataFrame with PCA results and words\n",
        "pca_df = pd.DataFrame(result, columns=['x', 'y'])\n",
        "pca_df['word'] = words\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "fig = go.Figure(data=go.Scattergl(\n",
        "    x=pca_df['x'],\n",
        "    y=pca_df['y'],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        colorscale='Viridis',\n",
        "        line_width=1\n",
        "    ),\n",
        "    text=pca_df['word'],\n",
        "    textposition=\"bottom center\"\n",
        "))\n",
        "\n",
        "fig.show()\n",
        "actual_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\n",
        "actual_neg_filt =  df['topn_semantic_sentiment_polarity'] == 0\n",
        "\n",
        "# filter positive and negative review based on Most Probable predicted 'y' or 'topn_semantic_sentiment_score' column\n",
        "predicted_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\n",
        "predicted_neg_filt = df['topn_semantic_sentiment_polarity'] == 0\n",
        "\n",
        "\n",
        "\n",
        "# plotting Semantic Sentiment Score Position of Actual Negative Reviews\n",
        "plt.scatter(df['topn_NSS'][actual_neg_filt],\n",
        "         df['topn_PSS'][actual_neg_filt],\n",
        "         label='Actual Negetive Reviews',\n",
        "           color='DarkRed',\n",
        "            alpha=0.4 , # set transparency of color\n",
        "            s=20 # set size of dots\n",
        "           )\n",
        "\n",
        "# plotting Semantic Sentiment Score Position of Actual Positive Reviews\n",
        "plt.scatter(df['topn_NSS'][actual_pos_filt],\n",
        "         df['topn_PSS'][actual_pos_filt],\n",
        "         label='Actual Positive Reviews',\n",
        "       color='DarkGreen',\n",
        "            alpha=0.1, # set transparency of color\n",
        "            s=30 # set size of dots\n",
        "           )\n",
        "# naming the x & y axis\n",
        "plt.xlabel('Predicted Negative Labels')\n",
        "plt.ylabel('Predicted Positive Labels')\n",
        "Text(0, 0.5, 'Predicted Positive Labels')\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "UfWszkmaoHXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Downloading the sentiment analysis model\n",
        "SentimentClassifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Downloading the sentiment analysis model\n",
        "# SentimentClassifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "metadata": {
        "id": "j2R4TbWtoL4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZErW02ZoOoY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}